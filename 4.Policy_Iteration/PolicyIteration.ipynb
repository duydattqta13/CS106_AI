{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20520435.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thử nghiệm với FrozenLake-v0"
      ],
      "metadata": {
        "id": "A9_qFGPXaaax"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "753865d7-a43c-47bf-ce70-f99f38f71263"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "2555487d-7514-4262-b038-9190a5e8e254"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "ce998d2e-5595-4be1-dc1e-04da090f60a3"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            env.render()\n",
        "            time.sleep(0.2)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "metadata": {
        "id": "ysLKetsdaPub"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "            \n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "                \n",
        "                q_values.append(q_value)\n",
        "            \n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "        \n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "        \n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            #print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "    \n",
        "    return v_values"
      ],
      "metadata": {
        "id": "QRNigyDTQIHC"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "            \n",
        "            q_values.append(q_value)\n",
        "        \n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "    \n",
        "    return policy"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # Init policy with all action is zero \n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # Repeat until converged\n",
        "    for i in range(max_iters):\n",
        "        # evaluate current policy by policy_evaluation\n",
        "        v_values = policy_evaluation(env=env, policy=policy)\n",
        "        # one-step look ahead with policy_extraction\n",
        "        new_policy = policy_extraction(env, v_values=v_values)\n",
        "\n",
        "        # if new policy is the same as previous one then it is converged, we should break\n",
        "        if np.all(policy == new_policy):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "        else:\n",
        "            policy = new_policy\n",
        "    return policy"
      ],
      "metadata": {
        "id": "e5NY3HC-kkme"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xAljw7VuMP",
        "outputId": "075edfd4-402b-4d78-9be5-4a53e3d75cca"
      },
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Policy iteration:')\n",
        "time_start = time.time()\n",
        "pi_policy = policy_iteration(env)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env=env, policy=pi_policy, max_episodes=1000)\n",
        "print('-------------------------------------------------------------')\n",
        "print('Value iteration:')\n",
        "time_start = time.time()\n",
        "optimal_v_values = value_iteration(env)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdyhDpWqlASI",
        "outputId": "314b0ff5-7c3e-419a-9e07-e04e7f7f70b7"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy iteration:\n",
            "Converged at 5-th iteration.\n",
            "Time: 0.04954791069030762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 758/1000\n",
            "Average number of steps: 36.193931398416886\n",
            "-------------------------------------------------------------\n",
            "Value iteration:\n",
            "Converged at 79-th iteration.\n",
            "Time: 0.04405808448791504\n",
            "Number of successes: 730/1000\n",
            "Average number of steps: 38.00821917808219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-m4ZqWZXKqG",
        "outputId": "c6b58d7c-b308-465e-f22e-0ae48fc870a2"
      },
      "source": [
        "optimal_policy\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 764/1000\n",
            "Average number of steps: 38.40575916230367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thử nghiệm với FrozenLake8x8-v0"
      ],
      "metadata": {
        "id": "dYU2D27la1jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('FrozenLake8x8-v0')"
      ],
      "metadata": {
        "id": "sFTciksUbBhO"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKNw-ZAIbDDX",
        "outputId": "2414cf05-685a-4e88-82ab-43af18e0fd05"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl5pUDdUbJ33",
        "outputId": "3f0d19f2-7b84-4b63-9206-5985d60c309e"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Policy iteration:')\n",
        "time_start = time.time()\n",
        "pi_policy = policy_iteration(env)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env=env, policy=pi_policy, max_episodes=1000)\n",
        "print('-------------------------------------------------------------')\n",
        "print('Value iteration:')\n",
        "time_start = time.time()\n",
        "optimal_v_values = value_iteration(env)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dC8ROkOtbQhH",
        "outputId": "7f87465d-ede0-4594-fa50-c03712f03cda"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy iteration:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 9-th iteration.\n",
            "Time: 0.2327895164489746\n",
            "Number of successes: 731/1000\n",
            "Average number of steps: 73.20656634746922\n",
            "-------------------------------------------------------------\n",
            "Value iteration:\n",
            "Converged at 117-th iteration.\n",
            "Time: 0.27042412757873535\n",
            "Number of successes: 723/1000\n",
            "Average number of steps: 74.25034578146611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thử nghiệm với Taxi-v3"
      ],
      "metadata": {
        "id": "Rau4_gV9a7nT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Taxi-v3')"
      ],
      "metadata": {
        "id": "20lulv5HbfKh"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iasev9FpbhiR",
        "outputId": "fcda5c3f-0956-4b55-a6fe-3d20243ef71e"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JqcVFFjbjPi",
        "outputId": "c364a8f0-0911-416e-fbd6-93627c7fe950"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Policy iteration:')\n",
        "time_start = time.time()\n",
        "pi_policy = policy_iteration(env)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env=env, policy=pi_policy, max_episodes=1000)\n",
        "print('-------------------------------------------------------------')\n",
        "print('Value iteration:')\n",
        "time_start = time.time()\n",
        "optimal_v_values = value_iteration(env)\n",
        "optimal_policy = policy_extraction(env, optimal_v_values)\n",
        "time_end = time.time()\n",
        "print(f'Time: {time_end - time_start}')\n",
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "metadata": {
        "id": "iaQ6o6znbuId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd8f05f4-2ec0-4d25-a779-87ec3c6a0e63"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy iteration:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 16-th iteration.\n",
            "Time: 2.313659906387329\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.009\n",
            "-------------------------------------------------------------\n",
            "Value iteration:\n",
            "Converged at 116-th iteration.\n",
            "Time: 1.1200292110443115\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nhận xét"
      ],
      "metadata": {
        "id": "zGZb0tNtcijP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-Số bước hội tụ của Value iteration và Policy Iteration lần lượt là 79 và 5 (FronzenLake-v0), 117 và 9 (FrozenLake8x8-v0), 116 và 6 (Taxi-v3)\n",
        "-Number of successes của Value iteration và Policy Iteration qua các env gần như giống nhau"
      ],
      "metadata": {
        "id": "SXOywg2InYqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Value iteration:\n",
        "  + Chi phí tính toán đắt hơn Policy iteration\n",
        "  + yêu cầu nhiều iterations để hội tụ\n",
        "\n",
        "- Policy iteration:\n",
        "  + Chi phí tính toán rẻ hơn Value iteration\n",
        "  + yêu cầu ít iterations hơn để hội tụ\n"
      ],
      "metadata": {
        "id": "2kaDG9gAb-bJ"
      }
    }
  ]
}